# 📄 DocuSense AI

DocuSense AI is a fully self-contained, Flask-based question-answering system that transforms static PDF documents into interactive knowledge bases without relying on any external APIs. At its core, DocuSense ingests user-uploaded PDFs via a minimalist upload button in the header, extracts raw text using PyMuPDF, and partitions it into overlapping, context-preserving chunks. Each chunk is then vectorized with a lightweight, high-throughput embedding model (Sentence-Transformers all-MiniLM-L6-v2) and stored in an in-memory FAISS index, enabling sub-second semantic similarity searches across potentially thousands of passages.

When a user poses a natural-language query, the backend performs a top-K nearest-neighbor lookup in FAISS, retrieves the most relevant text snippets, and assembles them into a single contextual prompt. This prompt is forwarded to a locally hosted LLaMA 2 model running under Ollama’s Docker-powered inference engine. As the model streams its generated answer, the frontend gradually renders the response in the chat pane, giving users immediate feedback and highlighting which document passages were cited. This retrieval-augmented generation (RAG) approach ensures that every answer is both grounded in the source material and enriched by the LLM’s language fluency.

The user interface strikes a balance between simplicity and functionality: a two-panel layout shows the scrollable PDF viewer on the left and the question-and-answer chat interface on the right. All UI components are built with Bootstrap 5 for responsive behavior on desktop and mobile devices alike. Every document thumbnail is automatically generated and displayed on the landing page, where users can quickly switch between different PDFs or upload new ones. Under the hood, thumbnails are rendered via PyMuPDF and Pillow, then served as static assets alongside the Flask application.

DocuSense’s modular architecture makes it easy to swap out or extend any piece of the pipeline. Embedding models can be replaced simply by changing a model name in the embeddings.py module; the FAISS vector store can be swapped for other open-source stores like Chroma or Milvus; and the Ollama-driven LLaMA 2 inference can be redirected to any other local or remote LLM. With no external dependencies beyond open-source libraries and local resources, DocuSense AI offers a transparent, secure, and highly customizable foundation for building document-centric conversational agents.

---

## 🚀 Features

- 📤 **Upload** a PDF  
- 🔍 **Ask questions** about the document  
- 🧠 **Semantic chunk retrieval** using FAISS + MiniLM embeddings  
- 🤖 **LLM-powered answers** with Ollama + LLaMA 2  
- 🔒 **Fully local & open-source** (no API keys)

---

## 🖥️ Pages

### 1. Landing Page  
![Landing Page](images/Landing%20Page.png)  
Users start here: click the **+** button in the header to upload a PDF, or select an existing thumbnail to begin.

### 2. Q&A Screen  
![Q&A Screen](images/QnA%20Screen.png)  
Once a PDF is loaded, ask questions in natural language; answers are generated by the RAG pipeline.

---

## 🛠️ Tech Stack

- **Frontend**: HTML (Flask templates), Bootstrap 5  
- **Backend**: Python, Flask  
- **LLM Inference**: Ollama + LLaMA 2  
- **Embeddings**: Sentence-Transformers (`all-MiniLM-L6-v2`)  
- **Vector Search**: FAISS  
- **PDF Parsing**: PyMuPDF  
- **Tokenization**: tiktoken  

---

## 📦 Installation

1. **Clone** the repo  
   ```bash
   git clone https://github.com/yourusername/docusense-ai.git
   cd docusense-ai